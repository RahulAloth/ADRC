# ğŸš— ADRC â€” Autonomous Driving Remote Control

ADRC stands for **Autonomous Driving Remote Control**, a system designed to remotely manage autonomousâ€‘driving (AD) functionality using Edge AI. This project targets the testing and validation phase of **Levelâ€‘4 and Levelâ€‘5** autonomous vehicles, where safety, cost, and operational efficiency are critical.

---

## ğŸš€ Project Aim

The ADRC project enables **remote control and supervision** of autonomousâ€‘driving functions during highâ€‘level AD testing.  
In current OEM testing setups, a safety operator must sit inside the vehicle to:

- Manually enable or disable AD functions  
- Monitor the surroundings  
- Ensure AD activation only within validated test routes  
- Verify compliance with the Operational Design Domain (ODD)

This approach is safe but **not scalable** and **not costâ€‘efficient**.

ADRC replaces this inâ€‘vehicle operator with a **remote operator**, improving safety, efficiency, and scalability.

---

## ğŸ§  System Overview

A **Jetson device** is integrated with the vehicleâ€™s autonomousâ€‘driving stack. It performs two major functions:

### 1. **Remote AD Control**
- Communicates with a remote control center  
- Allows ADâ€‘enable/disable actions from a secure external location  
- Ensures AD activation only when conditions are safe and validated  

### 2. **Edge AI Perception**
- Uses a single camera to observe the road environment  
- Evaluates the situation similar to a human operator  
- Supports decisionâ€‘making during AD engagement  
- Uses GNSS data for precise vehicle positioning  

This ensures that ADâ€‘ready mode is activated **only on predefined, validated test paths**.

---

## ğŸ’¡ Why ADRC?

### âœ” Safety  
Testing Levelâ€‘4/Levelâ€‘5 systems without exposing passengers, pedestrians, or traffic to unnecessary risk.

### âœ” Cost Efficiency  
Eliminates the need for **two people** inside the vehicle (driver + operator).  
This significantly reduces testing costs across large fleets.

### âœ” Scalability  
Remote operators can supervise **multiple vehicles**, enabling largeâ€‘scale AD validation.

### âœ” Futureâ€‘Ready  
As the system matures, the remote operatorâ€™s role can be gradually reduced and eventually replaced by **Edgeâ€‘AIâ€‘based decision logic**, enabling:

- Autonomous ADâ€‘readiness checks  
- Automated ODD validation  
- Fully remote, AIâ€‘assisted AD testing workflows  

---

## ğŸ›  Key Components

- **NVIDIA Jetson (Xavier/Orin)**  
- **Edge AI perception pipeline**  
- **Singleâ€‘camera roadâ€‘scene understanding**  
- **GNSSâ€‘based ODD validation**  
- **Remote operator interface**  
- **Secure communication link to AD stack**  

---

## ğŸ§­ Testing Workflow

1. Vehicle enters a predefined test route  
2. Jetson evaluates the environment using camera + GNSS  
3. Remote operator receives live data  
4. Operator remotely enables ADâ€‘ready mode  
5. Vehicle activates Levelâ€‘4/Levelâ€‘5 functionality  
6. Safety driver remains as fallback only  
7. Over time, Edge AI can automate steps 2â€“4  

---

## ğŸ”­ Future Roadmap

- ROS2 integration  
- Multiâ€‘camera support  
- Redundant perception modules  
- Automated ODD compliance checks  
- Full Edgeâ€‘AIâ€‘based ADâ€‘readiness decision engine  
- Remote operator dashboard (webâ€‘based UI)  

---

## ğŸ“„ License
To be added based on your preference (MIT, Apacheâ€‘2.0, etc.)

---

## ğŸ¤ Contributions
Contributions, discussions, and suggestions are welcome.  
Currently I single handle this project with help of GPT 4. Once project is matured, I definetly invite people to collaberate. 


----


## âœ… Overview
This roadmap guides you through building an **Edge AI Remote Patrol Car** using:
- **Jetson Nano** for on-device AI
- **ROS2** for robotics middleware
- **NVIDIA DeepStream + TensorRT** for optimized inference
- **LiDAR + Camera** for mapping and perception

---

## ğŸ›  Prerequisites
- Jetson Nano (4GB recommended)
- MicroSD card (64GB or larger)
- Power supply (5V 4A)
- Ubuntu 18.04/20.04 PC for remote development
- Basic knowledge of Linux, ROS2, and Python

---

## ğŸ” Software Installation
### On Jetson Nano:

### Install ROS2 Humble/Foxy:
```bash
sudo apt update && sudo apt install ros-humble-desktop
```

### Install NVIDIA DeepStream SDK:
```bash
wget https://developer.nvidia.com/deepstream-sdk-download
sudo apt install deepstream-6.0
```

---

## ğŸ§© ROS2 Integration
1. Create a ROS2 workspace:
```bash
mkdir -p ~/ros2_ws/src
cd ~/ros2_ws
colcon build
```

2. Add nodes:
- **Teleop Node**: `teleop_twist_keyboard` or joystick.
- **Motor Control Node**: PWM signals to motor driver.
- **LiDAR Node**: RPLIDAR driver.
- **SLAM Node**: Cartographer or RTAB-Map.

---

## ğŸ¤– DeepStream AI Setup
1. Pull pre-trained model from NVIDIA NGC (e.g., PeopleNet):
```bash
wget https://ngc.nvidia.com/models/nvidia/peoplenet
```

2. Configure DeepStream pipeline:
- Camera input â†’ TensorRT inference â†’ ROS2 topic `/detections`.

3. Use GStreamer for video streaming:
```bash
gst-launch-1.0 nvarguscamerasrc ! nvv4l2h264enc ! rtph264pay ! udpsink host=<IP> port=5000
```

---

## ğŸ•¹ Teleoperation
- Use ROS2 `teleop_twist_keyboard`:
```bash
ros2 run teleop_twist_keyboard teleop_twist_keyboard
```

- Or joystick/gamepad via ROS2 Joy package.

---

## ğŸ—º Mapping
- Launch Cartographer for SLAM:
```bash
ros2 launch cartographer_ros cartographer.launch.py
```

- Visualize map in RViz:
```bash
ros2 run rviz2 rviz2
```

---

## ğŸ“¡ Streaming & Dashboard
- Optional: WebRTC for browser-based control.
- Or use GStreamer RTP for low-latency video.

---

## âœ… Testing & Optimization
- Validate teleop and mapping.
- Optimize TensorRT inference for Nano.
- Add safety features (watchdog, emergency stop).

---

## ğŸŒ Future Enhancements
- Autonomous navigation with waypoints.
- Multi-camera support.
- Integration with Edge AI analytics.
